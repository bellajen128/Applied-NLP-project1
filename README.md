# Slangify Project

è‡ªå‹•å°‡å¥å­ä¸­çš„è©æ›¿æ›æˆä¿šèªçš„ NLP ç³»çµ±ã€‚

## ğŸ“ å°ˆæ¡ˆçµæ§‹
```
NLP_latest/
â”œâ”€â”€ data/                          # è³‡æ–™æª”
â”‚   â”œâ”€â”€ raw/                       # åŸå§‹è³‡æ–™ (ud_2015-2025.csv)
â”‚   â”œâ”€â”€ slang_raw_combined.csv     # åˆä½µåŸå§‹è³‡æ–™ (59163 æ¢)
â”‚   â””â”€â”€ slang_clean_final.csv      # æ¸…ç†å¾Œè³‡æ–™ (9173 æ¢) â­
â”‚
â”œâ”€â”€ models/                        # è¨“ç·´å¥½çš„æ¨¡å‹
â”‚   â””â”€â”€ best_slang_bert_classifier.pt  # BERT Classifier (F1: 0.9858) â­
â”‚
â”œâ”€â”€ scripts/                       # ä¸»è¦è…³æœ¬
â”‚   â”œâ”€â”€ official_preprocessing.py  # è³‡æ–™æ¸…ç† pipeline
â”‚   â”œâ”€â”€ train_classifier.py        # è¨“ç·´ BERT Classifier
â”‚   â””â”€â”€ test_baseline_clean.py     # æ¸¬è©¦ Baseline ç³»çµ±
â”‚
â”œâ”€â”€ training/                      # è¨“ç·´ç›¸é—œæª”æ¡ˆ
â”‚   â”œâ”€â”€ training_data_clean.json   # BERT è¨“ç·´è³‡æ–™ (6000 æ¨£æœ¬) â­
â”‚   â””â”€â”€ training_history_*.json    # è¨“ç·´æ­·å²è¨˜éŒ„
â”‚
â””â”€â”€ archive/                       # èˆŠç‰ˆ/æ¸¬è©¦æª”æ¡ˆ
    â”œâ”€â”€ processed_slang_data.csv   # èˆŠç‰ˆè³‡æ–™
    â”œâ”€â”€ training_data.json         # èˆŠç‰ˆè¨“ç·´è³‡æ–™
    â”œâ”€â”€ test_*.py                  # å„ç¨®æ¸¬è©¦è…³æœ¬
    â””â”€â”€ ...
```

## ğŸ¯ Baseline ç³»çµ±æ¶æ§‹
```
è¼¸å…¥å¥å­
    â†“
[1] é—œéµè©æå– (spaCy)
    â†’ æå–å¯æ›¿æ›è© + è©æ€§æ¨™è¨»
    â†’ éæ¿¾ SKIP_WORDS (é»‘åå–®åŠŸèƒ½è©)
    â†’ æ’åº: ADJ > VERB > NOUN
    â†“
[2] FAISS æª¢ç´¢ (é è¨“ç·´ all-MiniLM-L6-v2)
    â†’ ç‚ºæ¯å€‹é—œéµè©æœå°‹å€™é¸
    â†’ éæ¿¾åƒåœ¾è© (NER, çœŸå¯¦è©æª¢æŸ¥, çµ±è¨ˆç‰¹å¾µ)
    â†“
[3] BERT Classifier è©•åˆ† (è¨“ç·´å¥½çš„ DistilBERT)
    â†’ åˆ¤æ–· (sentence, slang) é…å°é©é…æ€§
    â†“
[4] Combined Score
    â†’ 0.35 Ã— FAISS + 0.65 Ã— BERT + Bonus
    â†’ Bonus: POS + Match + Popularity (ç™½åå–® +0.25)
    â†“
[5] é¸æ“‡æœ€ä½³å€™é¸ä¸¦æ›¿æ›
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. è³‡æ–™æ¸…ç†
```bash
python scripts/official_preprocessing.py \
    --input data/slang_raw_combined.csv \
    --output data/slang_clean_final.csv \
    --min_quality 4
```

**è¼¸å‡ºï¼š** 9173 æ¢é«˜å“è³ª slang

---

### 2. è¨“ç·´ BERT Classifier
```bash
# å…ˆåœ¨ Jupyter ç”Ÿæˆè¨“ç·´è³‡æ–™ (training_data_clean.json)
# ç„¶å¾Œè¨“ç·´ï¼š

python scripts/train_classifier.py \
    --data training/training_data_clean.json \
    --epochs 3 \
    --batch_size 16
```

**è¼¸å‡ºï¼š** models/best_slang_bert_classifier.pt (Val F1: 0.9858)

---

### 3. æ¸¬è©¦ç³»çµ±
```bash
python scripts/test_baseline_clean.py
```

**æ¸¬è©¦çµæœï¼š**
- new â†’ fresh â­
- amazing â†’ lit af â­
- upset â†’ salty â­
- leave â†’ bounce â­
- suspicious â†’ sus â­

---

## ğŸ“Š ç³»çµ±æ•ˆèƒ½

| æŒ‡æ¨™ | æ•¸å€¼ |
|------|------|
| è³‡æ–™é‡ | 9173 æ¢ä¹¾æ·¨ slang |
| BERT Val F1 | 0.9858 |
| æ¸¬è©¦æˆåŠŸç‡ | 100% (5/5) |
| æ¨ç†é€Ÿåº¦ | ~0.5s per query |

---

## ğŸ”§ é…ç½®åƒæ•¸

### FAISS Retrieval
- `k_per_keyword`: 5 (æ¯å€‹é—œéµè©å– 5 å€‹å€™é¸)
- `min_faiss_score`: 0.25

### BERT Classifier
- `alpha`: 0.35 (FAISS vs BERT æ¬Šé‡)
- `conf_threshold`: 0.55

### Bonus Scores
- POS: ADJ +0.15, NOUN +0.05, VERB +0.0
- Match: Definition åŒ¹é… +0.15
- Popularity: ç™½åå–® +0.25

---

## ğŸ“ é‡è¦æª”æ¡ˆèªªæ˜

### è³‡æ–™æª”
- `slang_clean_final.csv` - **æœ€çµ‚ä½¿ç”¨çš„ä¹¾æ·¨è³‡æ–™** â­
- `slang_raw_combined.csv` - åŸå§‹åˆä½µè³‡æ–™ï¼ˆå‚™ä»½ç”¨ï¼‰

### æ¨¡å‹æª”
- `best_slang_bert_classifier.pt` - **è¨“ç·´å¥½çš„ BERT Classifier** â­

### è¨“ç·´è³‡æ–™
- `training_data_clean.json` - **BERT è¨“ç·´è³‡æ–™** â­
  - 3000 æ­£æ¨£æœ¬
  - 1500 Hard Negative
  - 1500 Easy Negative

---

## ğŸ¯ ä¸‹ä¸€æ­¥é–‹ç™¼

- [ ] äº’å‹•å¼å¤šè©æ›¿æ›æ¨¡çµ„
- [ ] Streamlit UI ä»‹é¢
- [ ] éƒ¨ç½²åˆ°é›²ç«¯

---

## ğŸ“š åƒè€ƒæ–‡ä»¶

- Preprocessing: `scripts/official_preprocessing.py`
- Training: `scripts/train_classifier.py`
- Testing: `scripts/test_baseline_clean.py`

