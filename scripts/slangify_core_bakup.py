"""
Slangify Core Module
æ•´åˆæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼Œå¯è¢« Streamlit æˆ–å…¶ä»–æ‡‰ç”¨ import

åŠŸèƒ½ï¼š
1. Baseline Pipeline (å–®ä¸€æœ€ä½³æ›¿æ›)
2. Interactive Mode (å¤šè©äº’å‹•æ›¿æ›)
"""

import torch
import faiss
import pandas as pd
import spacy
import re
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM # AutoModelForCausalLM ç”¨æ–¼ GPT-2
from pathlib import Path

# ============================================
# é…ç½®
# ============================================

SKIP_WORDS = {
    'soon', 'now', 'then', 'later', 'today', 'tomorrow', 'yesterday',
    'always', 'never', 'often', 'sometimes', 'already', 'still', 'recently',
    'need', 'want', 'will', 'would', 'could', 'should', 'must', 'can', 'may', 'might',
    'very', 'really', 'just', 'also', 'too', 'even', 'only', 'quite', 'pretty',
    'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'get', 'got', 'make', 'made',
    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
    'my', 'your', 'his', 'its', 'our', 'their',
    'the', 'a', 'an', 'this', 'that', 'these', 'those',
    'of', 'to', 'for', 'with', 'on', 'at', 'by', 'from', 'in', 'out',
    'because', 'but', 'and', 'or', 'so', 'if', 'when', 'while',
}

NAME_BLACKLIST = {
    'axel', 'nick', 'andreas', 'qwer', 'seehia', 'dad', 'trust',
    'natalie guest', 'kilt custard', 'squeet', 'gekyume'
}

POPULAR_SLANG = {
    'lit', 'fire', 'dope', 'sick', 'fresh', 'salty', 'sus', 'bounce', 'dip',
    'lit af', 'sus af', 'dead', 'fr', 'lowkey', 'highkey', 'bet', 'cap',
    'no cap', 'vibe', 'mood', 'slap', 'cringe', 'based', 'yeet', 'flex',
    'shady', 'throw', 'goat', 'bussin', 'finna', 'bruh', 'savage', 'stan',
    'tea', 'snatched', 'drip', 'woke', 'hype', 'legit', 'ez', 'gg'
}

POS_PRIORITY = {
    'ADJ': 0.15,
    'ADV': 0.10,
    'NOUN': 0.05,
    'VERB': 0.0
}


# ============================================
# Slangify System Class
# ============================================

class SlangifySystem:
    """
    å®Œæ•´çš„ Slangify ç³»çµ±
    
    åŠŸèƒ½ï¼š
    1. Baseline mode: è‡ªå‹•æ›¿æ›æœ€ä½³å€™é¸
    2. Interactive mode: å¤šè©äº’å‹•æ›¿æ›
    """
    
    def __init__(self, 
                 data_rel_path='data/slang_clean_final.csv',
                 model_rel_path='models/best_slang_bert_classifier.pt',
                 use_gpu=True):
        """
        åˆå§‹åŒ–ç³»çµ±
        
        Parameters:
        -----------
        data_rel_path : str
            Slang è³‡æ–™æª”çš„ç›¸å°è·¯å¾‘ (ç›¸å°æ–¼å°ˆæ¡ˆæ ¹ç›®éŒ„)
        model_rel_path : str
            è¨“ç·´å¥½çš„ BERT Classifier çš„ç›¸å°è·¯å¾‘
        use_gpu : bool
            æ˜¯å¦ä½¿ç”¨ GPU
        """
        print("="*60)
        print("Initializing Slangify System")
        print("="*60)
        
        # --- æª”æ¡ˆè·¯å¾‘ä¿®æ­£ (ä½¿ç”¨ Pathlib) ---
        # ç²å–ç•¶å‰è…³æœ¬çš„çµ•å°è·¯å¾‘
        current_script_path = Path(__file__).resolve()
        # å°ˆæ¡ˆæ ¹ç›®éŒ„ (å‡è¨­è…³æœ¬åœ¨ scripts è³‡æ–™å¤¾å…§)
        project_root = current_script_path.parent.parent 
        
        # æ§‹å»ºè³‡æ–™å’Œæ¨¡å‹çš„çµ•å°è·¯å¾‘
        data_path = project_root / data_rel_path
        model_path = project_root / model_rel_path
        # -----------------------------------
        
        # è¼‰å…¥ spaCy
        print("Loading spaCy...")
        self.nlp = spacy.load("en_core_web_md")
        
        # è¼‰å…¥è³‡æ–™
        print(f"Loading data: {data_path}")
        try:
            self.df = pd.read_csv(data_path)
            print(f"âœ… Loaded {len(self.df)} slang entries")
        except FileNotFoundError:
            print(f"âŒ ERROR: Data file not found at {data_path}")
            print("è«‹æª¢æŸ¥æ‚¨çš„è³‡æ–™è·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
            raise # é‡æ–°æ‹‹å‡ºéŒ¯èª¤
        
        # å»ºç«‹ FAISS ç´¢å¼•
        print("Building FAISS index...")
        self.retriever = SentenceTransformer("all-MiniLM-L6-v2")
        texts = self.df.apply(
            lambda r: f"{str(r['word'])} : {str(r['definition'])}", 
            axis=1
        ).tolist()
        embeddings = self.retriever.encode(texts, normalize_embeddings=True, show_progress_bar=True)
        self.index = faiss.IndexFlatIP(embeddings.shape[1])
        self.index.add(embeddings)
        print("âœ… FAISS index built")
        
        # è¼‰å…¥ BERT Classifier
        print(f"Loading BERT Classifier: {model_path}")
        self.device = torch.device('cuda' if (torch.cuda.is_available() and use_gpu) else 'cpu')
        self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "distilbert-base-uncased", 
            num_labels=2
        )
        
        try:
            # è¼‰å…¥æ¨¡å‹æ¬Šé‡
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            
            test_f1 = checkpoint.get('test_f1', checkpoint.get('val_f1', 0))
            print(f"âœ… BERT loaded (Test F1: {test_f1:.4f}, Device: {self.device})")
        except FileNotFoundError:
            print(f"âŒ ERROR: Model file not found at {model_path}")
            print("è«‹æª¢æŸ¥æ‚¨çš„æ¨¡å‹è·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
            raise
        except Exception as e:
             print(f"âŒ ERROR: Loading BERT failed: {e}")
             raise
         

        # --- æ–°å¢: è¼‰å…¥ GPT-2 Refiner ---
        print("Loading GPT-2 Refiner...")
        # é¸æ“‡ GPT-2 åŸºç¤æ¨¡å‹ã€‚æ³¨æ„ï¼šå¦‚æœé‡åˆ° OOM éŒ¯èª¤ï¼Œè«‹æ”¹ç”¨ 'distilgpt2'
        GPT2_MODEL_NAME = "distilgpt2" 
        self.gpt2_tokenizer = AutoTokenizer.from_pretrained(GPT2_MODEL_NAME)
        
        try:
            # ğŸš¨ é—œéµæ”¹è®Šï¼šé€™è£¡ç›´æ¥å°‡æ¨¡å‹è¨­å®šç‚º Noneï¼Œè·³é AutoModelForCausalLM çš„è¼‰å…¥
            self.gpt2_model = None 
            
            # æé†’ä½¿ç”¨è€… GPT-2 å·²ç¦ç”¨
            print("WARNING: GPT-2 model loading skipped due to Bus Error. Refinement will be disabled.")
            
        except Exception as e:
            # ç¢ºä¿éŒ¯èª¤è™•ç†é‚è¼¯ä»ç„¶å¥å£¯
            print(f"ERROR: Failed to load GPT-2 Model. Error: {e}")
            self.gpt2_model = None
        
        
        # try:
        #     self.gpt2_model = AutoModelForCausalLM.from_pretrained(GPT2_MODEL_NAME)
        #     if self.gpt2_tokenizer.pad_token is None:
        #         self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token
        #     self.gpt2_model.to(self.device)
        #     self.gpt2_model.eval()
        #     print("GPT-2 loaded.")
        # except Exception as e:
        #     # é€™æ˜¯é‡å° Streamlit æˆ– Slurm ç’°å¢ƒä¸­æ¨¡å‹è¼‰å…¥å¤±æ•—æ™‚çš„éŒ¯èª¤è™•ç†
        #     print(f"ERROR: Failed to load GPT-2 Model. This might cause high PPL scores. Error: {e}")
        #     self.gpt2_model = None # è¨­ç½®ç‚º None é¿å…å¾ŒçºŒèª¿ç”¨å ±éŒ¯
        # ---------------------------------
        
        
        print("\nâœ… System ready!\n")
    
    # ============================================
    # Baseline Mode Functions
    # ============================================
    
    def extract_keywords_with_pos(self, sentence):
        """
        æå–å¯æ›¿æ›çš„é—œéµè©ä¸¦æ¨™è¨»è©æ€§
        æŒ‰å„ªå…ˆç´šæ’åº: ADJ > VERB > NOUN
        """
        doc = self.nlp(sentence)
        keywords = []
        
        for token in doc:
            lemma = token.lemma_.lower()
            text_lower = token.text.lower()
            
            # é»‘åå–®éæ¿¾
            if lemma in SKIP_WORDS or text_lower in SKIP_WORDS:
                continue
            if len(token.text) <= 2 or token.is_stop:
                continue
            
            # åªä¿ç•™ ADJ/VERB/NOUN
            if token.pos_ in {'ADJ', 'VERB', 'NOUN'}:
                # è™•ç†ç‰‡èªå‹•è©
                if token.pos_ == "VERB":
                    phrase_lemma = lemma
                    phrase_text = text_lower
                    for child in token.children:
                        if child.dep_ == "prt":
                            phrase_lemma = f"{lemma} {child.text.lower()}"
                            phrase_text = f"{text_lower} {child.text.lower()}"
                            break
                    keywords.append((phrase_lemma, phrase_text, token.pos_, token.i))
                else:
                    keywords.append((lemma, text_lower, token.pos_, token.i))
        
        # æŒ‰è©æ€§å„ªå…ˆç´šæ’åº
        priority_map = {'ADJ': 0, 'VERB': 1, 'NOUN': 2}
        keywords.sort(key=lambda x: priority_map.get(x[2], 3))
        
        return keywords
    
    def retrieve_slang_faiss_by_keywords(self, sentence, keywords, 
                                        k_per_keyword=5, min_faiss_score=0.25):
        """
        ç‚ºæ¯å€‹é—œéµè©ä½¿ç”¨ FAISS æª¢ç´¢å€™é¸ slang
        """
        all_results = []
        seen_words = set()
        
        for kw_lemma, kw_text, kw_pos, kw_idx in keywords:
            # FAISS æª¢ç´¢
            q_emb = self.retriever.encode([kw_lemma], normalize_embeddings=True)
            scores, idxs = self.index.search(q_emb, k_per_keyword * 5)
            
            keyword_results = []
            for sc, ix in zip(scores[0], idxs[0]):
                if ix < 0 or ix >= len(self.df) or sc < min_faiss_score:
                    continue
                
                row = self.df.iloc[int(ix)]
                word = row["word"]
                word_lower = word.lower()
                definition = str(row["definition"]).lower()
                
                # éæ¿¾
                if word_lower in seen_words or word_lower in NAME_BLACKLIST or word_lower in SKIP_WORDS:
                    continue
                if len(word) >= 15 or word.count(' ') >= 2:
                    continue
                if word[0].isupper() and not (word.isupper() and len(word) <= 5):
                    continue
                if '-' in word and ' ' in word:
                    continue
                if any(char in word for char in '[]{}()|<>@#$%^&*+=~`'):
                    continue
                if word_lower.endswith(('ster', 'sters', 'son', 'man', 'mann')):
                    if word_lower not in {'gangster', 'hipster', 'youngster', 'monster'}:
                        continue
                if len(word) > 5:
                    vowel_count = sum(1 for c in word_lower if c in 'aeiou')
                    if vowel_count / len(word) < 0.15:
                        continue
                if word_lower == kw_lemma or word_lower == kw_text:
                    continue
                if kw_lemma in word_lower or word_lower in kw_lemma:
                    continue
                
                # åŒ¹é…åˆ†æ•¸
                pattern = rf"^{re.escape(kw_lemma)}[\s,;]"
                if re.match(pattern, definition):
                    match_score = 1.0
                elif re.search(rf"\b{re.escape(kw_lemma)}\b", definition):
                    match_score = 0.6
                else:
                    match_score = 0.3
                
                keyword_results.append({
                    "score": float(sc),
                    "word": word,
                    "definition": row["definition"],
                    "example": row.get("example", ""),
                    "matched_keyword": kw_lemma,
                    "original_word": kw_text,
                    "original_pos": kw_pos,
                    "original_idx": kw_idx,
                    "match_score": match_score,
                })
                seen_words.add(word_lower)
                
                if len(keyword_results) >= k_per_keyword:
                    break
            
            all_results.extend(keyword_results)
        
        return all_results
    
    def score_candidates_with_classifier(self, sentence, candidates, alpha=0.35):
        """
        ä½¿ç”¨ BERT Classifier è©•åˆ†ä¸¦è¨ˆç®— Combined Score
        """
        if not candidates:
            return []
        
        # BERT è©•åˆ†
        inputs = self.tokenizer(
            [sentence] * len(candidates),
            [f"{c['word']} : {c['definition']}" for c in candidates],
            padding=True, truncation=True, max_length=128, return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()
        
        # è¨ˆç®— Combined Score
        for i, c in enumerate(candidates):
            c["bert_score"] = float(probs[i])
            
            # åŸºç¤åˆ†æ•¸
            base_score = alpha * c["score"] + (1 - alpha) * c["bert_score"]
            
            # è©æ€§åŠ åˆ†
            pos_bonus = POS_PRIORITY.get(c.get("original_pos", ""), 0)
            
            # Definition åŒ¹é…åŠ åˆ†
            match_bonus = 0.15 if c.get("match_score", 0) >= 1.0 else \
                         0.08 if c.get("match_score", 0) >= 0.6 else 0
            
            # ç™½åå–®åŠ åˆ†
            c['is_popular'] = (c["word"].lower() in POPULAR_SLANG)
            popularity_bonus = 0.25 if c['is_popular'] else 0.05
            
            c["combined"] = base_score + pos_bonus + match_bonus + popularity_bonus
        
        # æ’åº
        candidates.sort(key=lambda x: x["combined"], reverse=True)
        return candidates
    
    def smart_replace(self, sentence, slang_word, original_word=None, original_idx=None):
        """æ™ºèƒ½æ›¿æ›"""
        doc = self.nlp(sentence)
        tokens = [token.text for token in doc]
        
        # æ–¹æ³•1: ä½¿ç”¨ index
        if original_idx is not None and 0 <= original_idx < len(tokens):
            tokens[original_idx] = slang_word
            return self._rebuild_sentence(doc, tokens)
        
        # æ–¹æ³•2: ç‰‡èªå‹•è©
        if original_word and ' ' in original_word:
            pattern = r'\b' + r'\s+'.join(re.escape(p) for p in original_word.split()) + r'\b'
            result, count = re.subn(pattern, slang_word, sentence, flags=re.IGNORECASE)
            if count > 0:
                return result
        
        # æ–¹æ³•3: å–®è©åŒ¹é…
        if original_word:
            for i, token in enumerate(doc):
                if token.text.lower() == original_word or token.lemma_.lower() == original_word:
                    tokens[i] = slang_word
                    return self._rebuild_sentence(doc, tokens)
        
        return sentence
    
    def _rebuild_sentence(self, doc, tokens):
        """é‡å»ºå¥å­ï¼ˆä¿ç•™æ¨™é»ç¬¦è™Ÿé–“è·ï¼‰"""
        result = []
        for i, token in enumerate(doc):
            if i > 0 and not token.is_punct and not tokens[i-1].endswith("'"):
                result.append(" ")
            result.append(tokens[i])
        return "".join(result)
    
    # ============================================
    # GPT-2 Refinement Function
    # ============================================

    def refine_with_gpt2(self, original_sentence: str, slangified_sentence: str) -> str:
        """
        ä½¿ç”¨ GPT-2 é€²è¡Œæ•´å¥æ”¹å¯«ï¼Œä½¿å…¶æ›´è‡ªç„¶æµæš¢ã€‚
        """
        # å¦‚æœ GPT-2 è¼‰å…¥å¤±æ•—ï¼Œå‰‡ç›´æ¥è¿”å›æœªç²¾ç…‰çš„å¥å­
        if self.gpt2_model is None:
            return slangified_sentence
        
        prompt = (
            f"Original sentence: {original_sentence}\n"
            f"Slangified sentence: {slangified_sentence}\n"
            "Task: Rewrite the slangified sentence to sound more natural and fluent while keeping the slang word/phrase.\n"
            "Rewritten sentence:"
        )

        inputs = self.gpt2_tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # é™åˆ¶ç”Ÿæˆé•·åº¦ï¼Œé˜²æ­¢æ¨¡å‹å¤±æ§
        max_len = inputs['input_ids'].shape[1] + 30 
        
        try:
            with torch.no_grad():
                output_tokens = self.gpt2_model.generate(
                    **inputs,
                    max_length=max_len,
                    num_return_sequences=1,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95,
                    temperature=0.7,
                    pad_token_id=self.gpt2_tokenizer.eos_token_id
                )
            
            output_text = self.gpt2_tokenizer.decode(output_tokens[0], skip_special_tokens=True)
            
            # æå– "Rewritten sentence:" ä¹‹å¾Œçš„éƒ¨åˆ†
            result = output_text.split("Rewritten sentence:")[1].strip()
            if '\n' in result:
                result = result.split('\n')[0].strip()
            return result
        except Exception as e:
            # å¦‚æœç”Ÿæˆå¤±æ•—ï¼Œè¿”å›åŸå§‹æ›¿æ›å¥
            print(f"Warning: GPT-2 refinement failed during generation. Error: {e}")
            return slangified_sentence
    
    # ============================================
    # Baseline Mode: è‡ªå‹•æ›¿æ›æœ€ä½³å€™é¸
    # ============================================
    
    def slangify(self, sentence, k_per_keyword=5, conf_threshold=0.55, alpha=0.35):
        """
        Baseline Pipeline: è‡ªå‹•é¸æ“‡æœ€ä½³ slang æ›¿æ›
        
        Parameters:
        -----------
        sentence : str
            è¼¸å…¥å¥å­
        k_per_keyword : int
            æ¯å€‹é—œéµè©å–å¹¾å€‹å€™é¸
        conf_threshold : float
            ä¿¡å¿ƒé–€æª»
        alpha : float
            FAISS vs BERT æ¬Šé‡
        
        Returns:
        --------
        tuple: (æ›¿æ›å¾Œçš„å¥å­, æœ€ä½³å€™é¸è©è³‡è¨Š)
        """
        # Step 1: æå–é—œéµè©
        keywords = self.extract_keywords_with_pos(sentence)
        if not keywords:
            return sentence, None
        
        # Step 2: FAISS æª¢ç´¢
        candidates = self.retrieve_slang_faiss_by_keywords(
            sentence, keywords, k_per_keyword, min_faiss_score=0.25
        )
        if not candidates:
            return sentence, None
        
        # Step 3: BERT è©•åˆ†
        candidates = self.score_candidates_with_classifier(sentence, candidates, alpha)
        
        # Step 4: é¸æ“‡æœ€ä½³
        best = candidates[0]
        if best["combined"] < conf_threshold:
            return sentence, None
        
        # Step 5: æ›¿æ›
        slangified_result = self.smart_replace(
            sentence, best["word"],
            best.get("original_word"),
            best.get("original_idx")
        )
        
        # Step 6: GPT-2 æ”¹å¯«æµæš¢åŒ– (æ–°å¢)
        # ä¸è«–æˆåŠŸèˆ‡å¦ï¼Œéƒ½æœƒè¿”å›ä¸€å€‹çµæœ
        refined_result = self.refine_with_gpt2(sentence, slangified_result)
        
        return refined_result, best # è¿”å› GPT-2 å„ªåŒ–å¾Œçš„çµæœ
    
    # ============================================
    # Interactive Mode: å¤šè©äº’å‹•æ›¿æ›
    # ============================================
    
    def analyze_sentence(self, sentence):
        """
        åˆ†æå¥å­ï¼Œè¿”å›æ‰€æœ‰å¯æ›¿æ›çš„è©
        
        Returns:
        --------
        list of dict: [
            {
                'text': 'stylish',
                'lemma': 'stylish',
                'pos': 'ADJ',
                'index': 4,
                'replaceable': True
            },
            ...
        ]
        """
        doc = self.nlp(sentence)
        tokens = []
        
        for token in doc:
            lemma = token.lemma_.lower()
            text_lower = token.text.lower()
            
            # åˆ¤æ–·æ˜¯å¦å¯æ›¿æ›
            replaceable = (
                token.pos_ in {'ADJ', 'ADV', 'VERB', 'NOUN'} and
                lemma not in SKIP_WORDS and
                text_lower not in SKIP_WORDS and
                len(token.text) > 2 and
                not token.is_stop
            )
            
            tokens.append({
                'text': token.text,
                'lemma': lemma,
                'pos': token.pos_,
                'index': token.i,
                'replaceable': replaceable
            })
        
        return tokens
    
    def get_suggestions(self, sentence, word_lemma, word_pos, word_index, top_k=5):
        """
        ç‚ºå–®ä¸€è©ç²å– slang å»ºè­°
        
        Parameters:
        -----------
        sentence : str
            å®Œæ•´å¥å­
        word_lemma : str
            è©çš„ lemma
        word_pos : str
            è©æ€§
        word_index : int
            è©åœ¨å¥å­ä¸­çš„ä½ç½®
        top_k : int
            è¿”å›å‰ k å€‹å»ºè­°
        
        Returns:
        --------
        list of dict: [
            {
                'slang': 'fresh',
                'definition': 'new and stylish',
                'example': 'Those kicks are fresh.',
                'score': 0.92,
                'faiss_score': 0.48,
                'bert_score': 0.95,
                'is_popular': True,
                'preview': 'His outfit is really fresh'
            },
            ...
        ]
        """
        # FAISS æª¢ç´¢
        q_emb = self.retriever.encode([word_lemma], normalize_embeddings=True)
        scores, idxs = self.index.search(q_emb, top_k * 5)
        
        candidates = []
        seen_words = set()
        
        for sc, ix in zip(scores[0], idxs[0]):
            if ix < 0 or ix >= len(self.df) or sc < 0.25:
                continue
            
            row = self.df.iloc[int(ix)]
            word = row["word"]
            word_lower = word.lower()
            
            # åŸºæœ¬éæ¿¾
            if word_lower in seen_words or word_lower in SKIP_WORDS:
                continue
            if word_lower == word_lemma or word_lemma in word_lower:
                continue
            
            candidates.append({
                "word": word,
                "definition": str(row["definition"]),
                "example": str(row.get("example", "")),
                "faiss_score": float(sc)
            })
            seen_words.add(word_lower)
            
            if len(candidates) >= top_k * 2:
                break
        
        if not candidates:
            return []
        
        # BERT è©•åˆ†
        inputs = self.tokenizer(
            [sentence] * len(candidates),
            [f"{c['word']} : {c['definition']}" for c in candidates],
            padding=True, truncation=True, max_length=128, return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()
        
        # Combined Score
        for i, c in enumerate(candidates):
            c["bert_score"] = float(probs[i])
            base_score = 0.35 * c["faiss_score"] + 0.65 * c["bert_score"]
            pos_bonus = POS_PRIORITY.get(word_pos, 0)
            match_bonus = 0.10 if word_lemma in c["definition"].lower() else 0
            c['is_popular'] = (c["word"].lower() in POPULAR_SLANG)
            popularity_bonus = 0.25 if c['is_popular'] else 0.05
            c["combined"] = base_score + pos_bonus + match_bonus + popularity_bonus
            c["score"] = round(c["combined"], 2)
        
        # æ’åº
        candidates.sort(key=lambda x: x["combined"], reverse=True)
        
        # ç”Ÿæˆé è¦½
        doc = self.nlp(sentence)
        for c in candidates[:top_k]:
            tokens = [t.text for t in doc]
            tokens[word_index] = c["word"]
            
            # 1. æ§‹å»ºåŸå§‹æ›¿æ›çš„é è¦½å¥
            raw_preview = self._rebuild_sentence(doc, tokens) 
            
            # 2. å°é è¦½å¥é€²è¡Œ GPT-2 æµæš¢åŒ– (ä½¿ç”¨ raw_preview)
            c['preview'] = self.refine_with_gpt2(sentence, raw_preview) 
            
            # æ¸…ç†
            c['slang'] = c.pop('word')
            del c['combined']
        
        return candidates[:top_k]
    
    def apply_replacements(self, sentence, replacements):
        """
        æ‡‰ç”¨å¤šå€‹æ›¿æ›
        
        Parameters:
        -----------
        sentence : str
            åŸå§‹å¥å­
        replacements : dict
            {word_index: slang_word}
        
        Returns:
        --------
        str: æ›¿æ›å¾Œçš„å¥å­
        """
        doc = self.nlp(sentence)
        tokens = [t.text for t in doc]
        
        # å¾å¤§åˆ°å°æ’åºï¼Œé¿å… index éŒ¯ä½
        for idx in sorted(replacements.keys(), reverse=True):
            if 0 <= idx < len(tokens):
                tokens[idx] = replacements[idx]
        
        return ' '.join(tokens)


# ============================================
# æ¸¬è©¦å‡½æ•¸
# ============================================

def test_system():
    """æ¸¬è©¦ç³»çµ±åŠŸèƒ½"""
    print("\n" + "="*60)
    print("Testing Slangify Core Module")
    print("="*60)
    
    # é€™è£¡çš„ SlangifySystem() æœƒä½¿ç”¨æ–°çš„çµ•å°è·¯å¾‘é‚è¼¯
    system = SlangifySystem()
    
    # Test 1: Baseline mode
    print("\n[Test 1] Baseline Mode (è‡ªå‹•æ›¿æ›)")
    print("-"*60)
    
    test_sentences = [
        "He likes to show off his new car.",
        "That party was really amazing last night.",
        "She is upset because she lost the game.",
    ]
    
    for sent in test_sentences:
        result, best = system.slangify(sent)
        if best:
            print(f"\nåŸå¥: {sent}")
            print(f"çµæœ: {result}")
            print(f"æ›¿æ›: {best['original_word']} â†’ {best['word']} (åˆ†æ•¸: {best['combined']:.2f})")
        else:
            print(f"\nåŸå¥: {sent}")
            print("çµæœ: ç„¡æ›¿æ›")
    
    # Test 2: Interactive mode
    print("\n\n[Test 2] Interactive Mode (å¤šè©æ›¿æ›)")
    print("-"*60)
    
    sentence = "His outfit is really stylish"
    print(f"\nå¥å­: {sentence}")
    
    # åˆ†æ
    tokens = system.analyze_sentence(sentence)
    replaceable = [t for t in tokens if t['replaceable']]
    
    print(f"\nå¯æ›¿æ›è©: {[t['text'] for t in replaceable]}")
    
    # ç‚º "stylish" ç²å–å»ºè­°
    # ç¢ºä¿ replaceable åˆ—è¡¨éç©ºå†è¨ªå•
    stylish_token = next((t for t in replaceable if t['lemma'] == 'stylish'), None)
    
    if stylish_token:
        suggestions = system.get_suggestions(
            sentence,
            stylish_token['lemma'],
            stylish_token['pos'],
            stylish_token['index'],
            top_k=3
        )
        
        print(f"\n'{stylish_token['text']}' çš„å»ºè­°:")
        for i, s in enumerate(suggestions, 1):
            star = "â­" if s['is_popular'] else ""
            print(f"  {i}. {s['slang']:12} ({s['score']:.2f}) {star} - {s['definition'][:40]}...")
            print(f"     é è¦½: {s['preview']}")
        
        # æ‡‰ç”¨æ›¿æ› (é€™è£¡å‡è¨­ 'really' çš„ index æ˜¯ 3)
        # ç‚ºäº†æ›´å®‰å…¨ï¼Œæˆ‘å€‘æ‡‰è©²ç”¨ lemma æŸ¥æ‰¾ index
        really_token = next((t for t in replaceable if t['lemma'] == 'really'), None)
        
        if really_token:
            replacements = {stylish_token['index']: 'chic', really_token['index']: 'fr'}  # stylish â†’ chic, really â†’ fr
            result = system.apply_replacements(sentence, replacements)
            
            print(f"\nå¤šè©æ›¿æ›:")
            print(f"  åŸå¥: {sentence}")
            print(f"  çµæœ: {result}")
        else:
            print("\n[SKIP] æ‰¾ä¸åˆ° 'really' è©ï¼Œè·³éå¤šè©æ›¿æ›æ¸¬è©¦ã€‚")
            
    else:
        print("\n[SKIP] æ‰¾ä¸åˆ° 'stylish' è©ï¼Œè·³é Interactive Mode æ¸¬è©¦ã€‚")


    print("\nâœ… All system initialization logic updated for robust path handling.")


if __name__ == "__main__":
    test_system()